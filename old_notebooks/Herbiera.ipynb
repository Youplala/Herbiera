{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herbiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:04:58.399001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-04 18:04:58.846763: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:04:58.846796: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-04 18:05:00.327526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:05:00.327619: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-04 18:05:00.327628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import des librairies\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import numpy as npx\n",
    "import keras\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import seaborn\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Images: 1/330\n",
      "Loading Images: 2/330\n",
      "Loading Images: 3/330\n",
      "Loading Images: 4/330\n",
      "Loading Images: 5/330\n",
      "Loading Images: 6/330\n",
      "Loading Images: 7/330\n",
      "Loading Images: 8/330\n",
      "Loading Images: 9/330\n",
      "Loading Images: 10/330\n",
      "Loading Images: 11/330\n",
      "Loading Images: 12/330\n",
      "Loading Images: 13/330\n",
      "Loading Images: 14/330\n",
      "Loading Images: 15/330\n",
      "Loading Images: 16/330\n",
      "Loading Images: 17/330\n",
      "Loading Images: 18/330\n",
      "Loading Images: 19/330\n",
      "Loading Images: 20/330\n",
      "Loading Images: 21/330\n",
      "Loading Images: 22/330\n",
      "Loading Images: 23/330\n",
      "Loading Images: 24/330\n",
      "Loading Images: 25/330\n",
      "Loading Images: 26/330\n",
      "Loading Images: 27/330\n",
      "Loading Images: 28/330\n",
      "Loading Images: 29/330\n",
      "Loading Images: 30/330\n",
      "Loading Images: 31/330\n",
      "Loading Images: 32/330\n",
      "Loading Images: 33/330\n",
      "Loading Images: 34/330\n",
      "Loading Images: 35/330\n",
      "Loading Images: 36/330\n",
      "Loading Images: 37/330\n",
      "Loading Images: 38/330\n",
      "Loading Images: 39/330\n",
      "Loading Images: 40/330\n",
      "Loading Images: 41/330\n",
      "Loading Images: 42/330\n",
      "Loading Images: 43/330\n",
      "Loading Images: 44/330\n",
      "Loading Images: 45/330\n",
      "Loading Images: 46/330\n",
      "Loading Images: 47/330\n",
      "Loading Images: 48/330\n",
      "Loading Images: 49/330\n",
      "Loading Images: 50/330\n",
      "Loading Images: 51/330\n",
      "Loading Images: 52/330\n",
      "Loading Images: 53/330\n",
      "Loading Images: 54/330\n",
      "Loading Images: 55/330\n",
      "Loading Images: 56/330\n",
      "Loading Images: 57/330\n",
      "Loading Images: 58/330\n",
      "Loading Images: 59/330\n",
      "Loading Images: 60/330\n",
      "Loading Images: 61/330\n",
      "Loading Images: 62/330\n",
      "Loading Images: 63/330\n",
      "Loading Images: 64/330\n",
      "Loading Images: 65/330\n",
      "Loading Images: 66/330\n",
      "Loading Images: 67/330\n",
      "Loading Images: 68/330\n",
      "Loading Images: 69/330\n",
      "Loading Images: 70/330\n",
      "Loading Images: 71/330\n",
      "Loading Images: 72/330\n",
      "Loading Images: 73/330\n",
      "Loading Images: 74/330\n",
      "Loading Images: 75/330\n",
      "Loading Images: 76/330\n",
      "Loading Images: 77/330\n",
      "Loading Images: 78/330\n",
      "Loading Images: 79/330\n",
      "Loading Images: 80/330\n",
      "Loading Images: 81/330\n",
      "Loading Images: 82/330\n",
      "Loading Images: 83/330\n",
      "Loading Images: 84/330\n",
      "Loading Images: 85/330\n",
      "Loading Images: 86/330\n",
      "Loading Images: 87/330\n",
      "Loading Images: 88/330\n",
      "Loading Images: 89/330\n",
      "Loading Images: 90/330\n",
      "Loading Images: 91/330\n",
      "Loading Images: 92/330\n",
      "Loading Images: 93/330\n",
      "Loading Images: 94/330\n",
      "Loading Images: 95/330\n",
      "Loading Images: 96/330\n",
      "Loading Images: 97/330\n",
      "Loading Images: 98/330\n",
      "Loading Images: 99/330\n",
      "Loading Images: 100/330\n",
      "Loading Images: 101/330\n",
      "Loading Images: 102/330\n",
      "Loading Images: 103/330\n",
      "Loading Images: 104/330\n",
      "Loading Images: 105/330\n",
      "Loading Images: 106/330\n",
      "Loading Images: 107/330\n",
      "Loading Images: 108/330\n",
      "Loading Images: 109/330\n",
      "Loading Images: 110/330\n",
      "Loading Images: 111/330\n",
      "Loading Images: 112/330\n",
      "Loading Images: 113/330\n",
      "Loading Images: 114/330\n",
      "Loading Images: 115/330\n",
      "Loading Images: 116/330\n",
      "Loading Images: 117/330\n",
      "Loading Images: 118/330\n",
      "Loading Images: 119/330\n",
      "Loading Images: 120/330\n",
      "Loading Images: 121/330\n",
      "Loading Images: 122/330\n",
      "Loading Images: 123/330\n",
      "Loading Images: 124/330\n",
      "Loading Images: 125/330\n",
      "Loading Images: 126/330\n",
      "Loading Images: 127/330\n",
      "Loading Images: 128/330\n",
      "Loading Images: 129/330\n",
      "Loading Images: 130/330\n",
      "Loading Images: 131/330\n",
      "Loading Images: 132/330\n",
      "Loading Images: 133/330\n",
      "Loading Images: 134/330\n",
      "Loading Images: 135/330\n",
      "Loading Images: 136/330\n",
      "Loading Images: 137/330\n",
      "Loading Images: 138/330\n",
      "Loading Images: 139/330\n",
      "Loading Images: 140/330\n",
      "Loading Images: 141/330\n",
      "Loading Images: 142/330\n",
      "Loading Images: 143/330\n",
      "Loading Images: 144/330\n",
      "Loading Images: 145/330\n",
      "Loading Images: 146/330\n",
      "Loading Images: 147/330\n",
      "Loading Images: 148/330\n",
      "Loading Images: 149/330\n",
      "Loading Images: 150/330\n",
      "Loading Images: 151/330\n",
      "Loading Images: 152/330\n",
      "Loading Images: 153/330\n",
      "Loading Images: 154/330\n",
      "Loading Images: 155/330\n",
      "Loading Images: 156/330\n",
      "Loading Images: 157/330\n",
      "Loading Images: 158/330\n",
      "Loading Images: 159/330\n",
      "Loading Images: 160/330\n",
      "Loading Images: 161/330\n",
      "Loading Images: 162/330\n",
      "Loading Images: 163/330\n",
      "Loading Images: 164/330\n",
      "Loading Images: 165/330\n",
      "Loading Images: 166/330\n",
      "Loading Images: 167/330\n",
      "Loading Images: 168/330\n",
      "Loading Images: 169/330\n",
      "Loading Images: 170/330\n",
      "Loading Images: 171/330\n",
      "Loading Images: 172/330\n",
      "Loading Images: 173/330\n",
      "Loading Images: 174/330\n",
      "Loading Images: 175/330\n",
      "Loading Images: 176/330\n",
      "Loading Images: 177/330\n",
      "Loading Images: 178/330\n",
      "Loading Images: 179/330\n",
      "Loading Images: 180/330\n",
      "Loading Images: 181/330\n",
      "Loading Images: 182/330\n",
      "Loading Images: 183/330\n",
      "Loading Images: 184/330\n",
      "Loading Images: 185/330\n",
      "Loading Images: 186/330\n",
      "Loading Images: 187/330\n",
      "Loading Images: 188/330\n",
      "Loading Images: 189/330\n",
      "Loading Images: 190/330\n",
      "Loading Images: 191/330\n",
      "Loading Images: 192/330\n",
      "Loading Images: 193/330\n",
      "Loading Images: 194/330\n",
      "Loading Images: 195/330\n",
      "Loading Images: 196/330\n",
      "Loading Images: 197/330\n",
      "Loading Images: 198/330\n",
      "Loading Images: 199/330\n",
      "Loading Images: 200/330\n",
      "Loading Images: 201/330\n",
      "Loading Images: 202/330\n",
      "Loading Images: 203/330\n",
      "Loading Images: 204/330\n",
      "Loading Images: 205/330\n",
      "Loading Images: 206/330\n",
      "Loading Images: 207/330\n",
      "Loading Images: 208/330\n",
      "Loading Images: 209/330\n",
      "Loading Images: 210/330\n",
      "Loading Images: 211/330\n",
      "Loading Images: 212/330\n",
      "Loading Images: 213/330\n",
      "Loading Images: 214/330\n",
      "Loading Images: 215/330\n",
      "Loading Images: 216/330\n",
      "Loading Images: 217/330\n",
      "Loading Images: 218/330\n",
      "Loading Images: 219/330\n",
      "Loading Images: 220/330\n",
      "Loading Images: 221/330\n",
      "Loading Images: 222/330\n",
      "Loading Images: 223/330\n",
      "Loading Images: 224/330\n",
      "Loading Images: 225/330\n",
      "Loading Images: 226/330\n",
      "Loading Images: 227/330\n",
      "Loading Images: 228/330\n",
      "Loading Images: 229/330\n",
      "Loading Images: 230/330\n",
      "Loading Images: 231/330\n",
      "Loading Images: 232/330\n",
      "Loading Images: 233/330\n",
      "Loading Images: 234/330\n",
      "Loading Images: 235/330\n",
      "Loading Images: 236/330\n",
      "Loading Images: 237/330\n",
      "Loading Images: 238/330\n",
      "Loading Images: 239/330\n",
      "Loading Images: 240/330\n",
      "Loading Images: 241/330\n",
      "Loading Images: 242/330\n",
      "Loading Images: 243/330\n",
      "Loading Images: 244/330\n",
      "Loading Images: 245/330\n",
      "Loading Images: 246/330\n",
      "Loading Images: 247/330\n",
      "Loading Images: 248/330\n",
      "Loading Images: 249/330\n",
      "Loading Images: 250/330\n",
      "Loading Images: 251/330\n",
      "Loading Images: 252/330\n",
      "Loading Images: 253/330\n",
      "Loading Images: 254/330\n",
      "Loading Images: 255/330\n",
      "Loading Images: 256/330\n",
      "Loading Images: 257/330\n",
      "Loading Images: 258/330\n",
      "Loading Images: 259/330\n",
      "Loading Images: 260/330\n",
      "Loading Images: 261/330\n",
      "Loading Images: 262/330\n",
      "Loading Images: 263/330\n",
      "Loading Images: 264/330\n",
      "Loading Images: 265/330\n",
      "Loading Images: 266/330\n",
      "Loading Images: 267/330\n",
      "Loading Images: 268/330\n",
      "Loading Images: 269/330\n",
      "Loading Images: 270/330\n",
      "Loading Images: 271/330\n",
      "Loading Images: 272/330\n",
      "Loading Images: 273/330\n",
      "Loading Images: 274/330\n",
      "Loading Images: 275/330\n",
      "Loading Images: 276/330\n",
      "Loading Images: 277/330\n",
      "Loading Images: 278/330\n",
      "Loading Images: 279/330\n",
      "Loading Images: 280/330\n",
      "Loading Images: 281/330\n",
      "Loading Images: 282/330\n",
      "Loading Images: 283/330\n",
      "Loading Images: 284/330\n",
      "Loading Images: 285/330\n",
      "Loading Images: 286/330\n",
      "Loading Images: 287/330\n",
      "Loading Images: 288/330\n",
      "Loading Images: 289/330\n",
      "Loading Images: 290/330\n",
      "Loading Images: 291/330\n",
      "Loading Images: 292/330\n",
      "Loading Images: 293/330\n",
      "Loading Images: 294/330\n",
      "Loading Images: 295/330\n",
      "Loading Images: 296/330\n",
      "Loading Images: 297/330\n",
      "Loading Images: 298/330\n",
      "Loading Images: 299/330\n",
      "Loading Images: 300/330\n",
      "Loading Images: 301/330\n",
      "Loading Images: 302/330\n",
      "Loading Images: 303/330\n",
      "Loading Images: 304/330\n",
      "Loading Images: 305/330\n",
      "Loading Images: 306/330\n",
      "Loading Images: 307/330\n",
      "Loading Images: 308/330\n",
      "Loading Images: 309/330\n",
      "Loading Images: 310/330\n",
      "Loading Images: 311/330\n",
      "Loading Images: 312/330\n",
      "Loading Images: 313/330\n",
      "Loading Images: 314/330\n",
      "Loading Images: 315/330\n",
      "Loading Images: 316/330\n",
      "Loading Images: 317/330\n",
      "Loading Images: 318/330\n",
      "Loading Images: 319/330\n",
      "Loading Images: 320/330\n",
      "Loading Images: 321/330\n",
      "Loading Images: 322/330\n",
      "Loading Images: 323/330\n",
      "Loading Images: 324/330\n",
      "Loading Images: 325/330\n",
      "Loading Images: 326/330\n",
      "Loading Images: 327/330\n",
      "Loading Images: 328/330\n",
      "Loading Images: 329/330\n",
      "Loading Images: 330/330\n"
     ]
    }
   ],
   "source": [
    "## Load Image\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (375, 500))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    rotation = cv2.rotate(img, cv2.ROTATE_180)\n",
    "    flip = cv2.flip(img, 1)\n",
    "    rotation_flip = cv2.flip(rotation, 1)\n",
    "    return img, rotation, flip, rotation_flip\n",
    "\n",
    "## Import des données\n",
    "def load_images_from_folder(folder):\n",
    "    train_x, train_y, test_x, test_y = ([], [], [], [])\n",
    "    aug_train_x, aug_train_y, aug_test_x, aug_test_y = ([], [], [], [])\n",
    "    count_files = 0\n",
    "    for _, _, files in os.walk(folder):\n",
    "        count_files += len(files)\n",
    "    count = 0\n",
    "    # Load Train Set\n",
    "    for specie in os.listdir(folder+'/Train'):\n",
    "        for image in os.listdir(folder+'/Train/'+specie):\n",
    "            count += 1\n",
    "            print('Loading Images: ' + str(count) + '/' + str(count_files))\n",
    "            img, rotation, flip, rotation_flip = load_image(folder+'/Train/'+specie+'/'+image)\n",
    "            if img is not None:\n",
    "                train_x.append(img)\n",
    "                aug_train_x.append(rotation)\n",
    "                aug_train_x.append(flip)\n",
    "                aug_train_x.append(rotation_flip)\n",
    "                train_y.append(specie)\n",
    "                aug_train_y.append(specie)\n",
    "                aug_train_y.append(specie)\n",
    "                aug_train_y.append(specie)\n",
    "    # Load Test Set\n",
    "    for specie in os.listdir(folder+'/Test'):\n",
    "        for image in os.listdir(folder+'/Test/'+specie):\n",
    "            count += 1\n",
    "            print('Loading Images: ' + str(count) + '/' + str(count_files))\n",
    "            img, rotation, flip, rotation_flip = load_image(folder+'/Test/'+specie+'/'+image)\n",
    "            if img is not None:\n",
    "                test_x.append(img)\n",
    "                aug_test_x.append(rotation)\n",
    "                aug_test_x.append(flip)\n",
    "                aug_test_x.append(rotation_flip)\n",
    "                test_y.append(specie)\n",
    "                aug_test_y.append(specie)\n",
    "                aug_test_y.append(specie)\n",
    "                aug_test_y.append(specie)\n",
    "    aug_test_x = test_x + aug_test_x\n",
    "    aug_test_y = test_y + aug_test_y\n",
    "    aug_train_x = train_x + aug_train_x\n",
    "    aug_train_y = train_y + aug_train_y\n",
    "    return train_x, train_y, test_x, test_y, aug_train_x, aug_train_y, aug_test_x, aug_test_y\n",
    "\n",
    "## Load images\n",
    "train_x, train_y, test_x, test_y, aug_train_x, aug_train_y, aug_test_x, aug_test_y = load_images_from_folder('dataset')\n",
    "# aug_train_y, aug_test_x, aug_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Afficher 9 images aléatoires\n",
    "def show_random_images(images, labels):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(9):\n",
    "        ax = fig.add_subplot(3, 3, i+1)\n",
    "        index = random.randint(0, len(images))\n",
    "        ax.imshow(images[index])\n",
    "        ax.set_title(labels[index])\n",
    "    plt.show()\n",
    "\n",
    "## Afficher les images\n",
    "# show_random_images(train_x, train_y)\n",
    "\n",
    "## Shuffle Dataset\n",
    "def shuffle(images, labels):\n",
    "    data = list(zip(list(images), list(labels)))\n",
    "    random.shuffle(data)\n",
    "    imgs = [i[0] for i in data]\n",
    "    lbls = [i[1] for i in data]\n",
    "    return imgs, lbls\n",
    "\n",
    "train_x, train_y = shuffle(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 498, 373, 4)       112       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 124, 93, 4)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 122, 91, 16)       592       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 22, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10560)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                675904    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 676,673\n",
      "Trainable params: 676,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:06:18.175249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-04 18:06:18.175476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175540: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175585: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175628: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175712: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175753: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/utility/Herbiera/venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-04 18:06:18.175801: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-04 18:06:18.177010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "28/28 [==============================] - 17s 586ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 15s 524ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 15s 526ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 15s 528ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 15s 552ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 15s 535ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 16s 580ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 16s 567ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 15s 527ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 15s 526ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 16s 585ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 16s 569ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 17s 605ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 16s 561ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 16s 563ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 16s 576ms/step - loss: 26.0000 - accuracy: 0.0909 - val_loss: 26.0000 - val_accuracy: 0.0909\n",
      "Epoch 17/50\n",
      "25/28 [=========================>....] - ETA: 1s - loss: 25.5387 - accuracy: 0.0887"
     ]
    }
   ],
   "source": [
    "## Création du modèle\n",
    "## Find the class of an image using a CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=3, activation='relu', input_shape=(500, 375, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "## Compilation du modèle\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "## Afficher le résumé du modèle\n",
    "model.summary()\n",
    "\n",
    "## convertir les labels en vecteurs\n",
    "def convert_labels(labels):\n",
    "    new_labels = []\n",
    "    new_labels_numbers = []\n",
    "    unique_labels = list(set(labels))\n",
    "    for label in labels:\n",
    "        vector = [0] * len(unique_labels)\n",
    "        vector[unique_labels.index(label)] = 1\n",
    "        new_labels.append(vector)\n",
    "        new_labels_numbers.append(unique_labels.index(label))\n",
    "    return new_labels_numbers\n",
    "    return new_labels\n",
    "\n",
    "# def convert_to_numeric(labels):\n",
    "#     unique = list(set(labels))\n",
    "#     numeric_labels = []\n",
    "#     for label in labels:\n",
    "#         numeric_labels.append([unique.index(label)])\n",
    "#     return numeric_labels\n",
    "\n",
    "## Fit du modèle\n",
    "model.fit(np.array(train_x), np.array(convert_labels(train_y)), epochs=50, batch_size=32,  validation_data=(np.array(test_x), np.array(convert_labels(test_y))))\n",
    "\n",
    "def analyze_preds(preds, labels):\n",
    "  confusion_matrix = sklearn.metrics.confusion_matrix(preds,\n",
    "                                                      labels,\n",
    "                                                      normalize=\"true\")\n",
    "  seaborn.heatmap(confusion_matrix,\n",
    "                  cmap=\"rocket_r\")\n",
    "  plt.title(\"Matrice de confusion\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "test_pred = np.argmax(model.predict(np.array(test_x)), axis=-1)\n",
    "analyze_preds(test_pred, np.argmax(np.array(convert_labels(test_y)), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05946141531cb8121b9621ff8287a006d9a7185868b75f000d35beedc9a2173e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
